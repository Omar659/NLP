{"cells":[{"cell_type":"markdown","source":["#Imports and Constants"],"metadata":{"id":"CjtPE8v_tTnk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21140,"status":"ok","timestamp":1684253354605,"user":{"displayName":"omar bayoumi","userId":"03666325538075971122"},"user_tz":-120},"id":"aoCHaI5jZuut","outputId":"5902eea1-04ef-4d96-e609-40fd03b967b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1337,"status":"ok","timestamp":1684253355937,"user":{"displayName":"omar bayoumi","userId":"03666325538075971122"},"user_tz":-120},"id":"GfDXea25zCMc","outputId":"9f0d2180-5ab0-4a5a-90a4-56b1c74d03f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'verbatlas'...\n","remote: Enumerating objects: 27, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 27 (delta 1), reused 17 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (27/27), 611.37 KiB | 2.77 MiB/s, done.\n"]}],"source":["!git clone \"https://github.com/Omar659/verbatlas.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4513,"status":"ok","timestamp":1684253360448,"user":{"displayName":"omar bayoumi","userId":"03666325538075971122"},"user_tz":-120},"id":"XNtE-C_AYSsA","outputId":"cbb8965d-14f8-49da-9da4-e3da357c7726"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# Nltk\n","from nltk.corpus import wordnet as wn\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","\n","# Utils\n","import json\n","import random\n","from collections import defaultdict\n","from collections import Counter\n","import csv\n","import itertools\n","import requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dA91A2cY5f_"},"outputs":[],"source":["# Paths\n","# Path to folder that contains files\n","ROOT_PATH = \"./drive/MyDrive/EAI/NLP/HW2/Project_NUANS/\"\n","# ROOT_PATH = \"./drive/MyDrive/Project_NUANS/\"\n","DATASETS_PATH = ROOT_PATH + \"Datasets/\"\n","OUTPUT_PATH = ROOT_PATH + \"Outputs/\""]},{"cell_type":"markdown","source":["#Functions and Classes"],"metadata":{"id":"tW26SGVhNAPo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684253360449,"user":{"displayName":"omar bayoumi","userId":"03666325538075971122"},"user_tz":-120},"id":"_L7CoUeKjcXY","outputId":"1a0086fd-7932-485a-ddd9-7168b0cf0c6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["=================================================================================\n","                                VerbAtlas 1.1.0\n","                             http://verbatlas.org\n","               Andrea Di Fabio, Simone Conia and Roberto Navigli\n","               Sapienza NLP Group, Sapienza University of Rome\n","                             http://nlp.uniroma1.it\n","               Web site and resource maintenance by Babelscape\n","                             http://babelscape.com\n","=================================================================================\n","VerbAtlas is a novel large-scale manually-crafted semantic resource for\n","wide-coverage, intelligible and scalable Semantic Role Labeling.\n","The goal of VerbAtlas is to manually cluster WordNet synsets that share similar\n","semantics into a set of semantically-coherent frames.\n","VerbAtlas is licensed under the CC BY-NC-SA 4.0 License.\n","=================================================================================\n","PACKAGE CONTENTS\n","=================================================================================\n","* README.txt (this file);\n","* LICENSES.txt (terms and conditions for the files provided in this package)\n","* VERBATLAS_LICENSE.txt (terms and conditions of the CC BY-NC-SA 4.0 License);\n","* WORDNET_LICENSE.txt (original license from WordNet 3.0);\n","* BABELNET_LICENSE.txt (original license from BabelNet 4.0);\n","* VerbAtlas-1.1.0/ (contains the files for VerbAtlas 1.0.3) \n","    * VA_frame_info.tsv      (name, id, and other info for each VerbAtlas frame)\n","    * VA_frame_pas.tsv       (the argument structure of each VerbAtlas frame)\n","    * VA_va2sp.tsv           (the selectional preferences of each VerbAtlas frame)\n","    * VA_bn2va.tsv           (the synsets in each VerbAtlas frame)\n","    * VA_preference_info.tsv (name, id, reference synset for each preference)\n","    * VA_bn2shadow.tsv       (shadow arguments for each synset)\n","    * VA_bn2implicit.tsv     (implicit arguments for each synset)\n","    * pb2va.tsv              (mapping from PropBank to VerbAtlas)\n","    * bn2wn.tsv              (one-to-one mapping from BabelNet to WordNet)\n","    * wn2lemma.tsv           (mapping from WordNet synset to lemmas)\n","    * wn2sense.tsv           (mapping from WordNet synset to WordNet sense key)\n","Every file in this package is licensed under the CC BY-NC-SA 4.0 License but \n","wn2lemma.tsv and wn2sense.tsv, which are licensed under the WordNet 3.0 License,\n","and bn2wn.tsv, which is licensed under the BabelNet 4.0 license.\n","=================================================================================\n","RESOURCES\n","=================================================================================\n","VerbAtlas relies on:\n","* WordNet 3.0 (https://wordnet.princeton.edu/download/current-version)\n","* BabelNet 4.0.1 (http://babelnet.org)\n","* Proposition Bank I (PropBank - http://propbank.github.io/)   \n","=================================================================================\n","FORMAT\n","=================================================================================\n","1. VA_frame_info.tsv\n","Each line contains the ID and the name of a VerbAtlas frame, separated by a tab.\n","From VerbAtlas 1.1.0, we also provide a definition, a prototypical synset (i.e.,\n","the synset that roughly represents the frame), the definition of the prototypical\n","synset, and a \"small definition\" of the frame.\n","Note: frames with id > 999 (e.g. va:1001f - AUXILIARY) are not \"semantic\" frames,\n","but rather \"supporting\" frames we added to support other resources like PropBank.\n","Please, do not consider them as core frames of VerbAtlas.\n","For example:\n","    va:0001f    TOLERATE         def-of-TOLERATE    bn:x   def-of-bn:x    small-def\n","    va:0002f    OBEY             def-of-OBEY        bn:y   def-of-bn:y    small-def\n","    va:0003f    CARRY_TRANSPORT  ...\n","    ...\n","2. VA_frame_pas.tsv\n","Each line contains the ID of a VerbAtlas frame and its argument structure, where\n","each element is separated by a tab.\n","For example:\n","    va:0001f    Agent    Theme    Beneficiary    Attribute\n","    va:0002f    Agent    Theme\n","    va:0003f    Agent    Theme    Destination    Source    Instrument ...\n","    ...\n","3. VA_va2sp.tsv\n","Each line contains the ID of a VerbAtlas frame F followed by the selectional\n","preferences of the roles in F. The default selectional preference for a role is\n","entity (id = va:0003p). If a role supports multiple selectional preferences,\n","they are separated by a pipe (|).\n","Some frames support two types of selectional preferences: Concrete (C) and\n","Abstract (A).\n","For example:\n","    va:0001f    C   Agent\n","    va:0002f\n","    va:0002f\n","    ...\n","4. VA_bn2va.tsv\n","Each line contains a BabelNet synset ID and its corresponding VerbAtlas frame ID,\n","separated by a tab.\n","For example:\n","    bn:00082138v    va:0001f\n","    bn:00082200v    va:0001f\n","    bn:00082203v    va:0001f\n","    ...\n","5. VA_preference_ids.tsv\n","Each line contains the ID of a VerbAtlas selectional preference, its reference\n","BabelNet synset ID, and its name.\n","For example:\n","    va:0001p    bn:00000467n    absorbent\n","    va:0002p    bn:00000492n    abstraction\n","    va:0003p    bn:00000902n    acid\n","    ...\n","6. VA_bn2shadow.tsv\n","Each line contains the shadow arguments, if any, for each BabelNet synset ID.\n","A synset may support multiple shadow arguments in different roles.\n","For example:\n","    bn:00082230v    Theme    bn:00070772n\n","    bn:00082124v    Stimulus    bn:00030466n\n","    bn:00082266v    Result    bn:00098546a    Instrument    bn:00000902n\n","    ...\n","7. VA_bn2implicit.tsv\n","Each line contains the implicit arguments, if any, for each BabelNet synset ID.\n","A synset may support multiple implicit arguments in different roles.\n","A role may have multiple implicit arguments, separated by a pipe (|).\n","For example:\n","    bn:00082144v    Patient    bn:00011769n\n","    bn:00082205v    Theme    bn:00067181n    Source    bn:00021015n\n","    bn:00082162v    Patient    bn:00035378n|bn:00035596n|bn:00036686n\n","    ...\n","8. pb2va.tsv\n","A mapping from PropBank to VerbAtlas. Each line maps a PropBank predicate sense\n","and its corresponding argument structure to a VerbAtlas frame and its argument\n","structure as follows:\n","[PB predicate sense]>[VA frame]    [PB role]>[VA role]    ...     \n","For example:\n","    abandon.01>va:0255f\n","    abandon.02>va:0016f\n","    abandon.03>va:0253f\n","    ...\n","9. bn2wn.tsv\n","A mapping from BabelNet synset to WordNet synset for verbs. Each line contains a\n","BabelNet synset ID and its corresponding WordNet synset ID, separated by a tab.\n","For example:\n","    bn:00082116v    wn:00865776v\n","    bn:00082117v    wn:02168378v\n","    bn:00082118v    wn:02228031v\n","    ...\n","10. wn2lemma.tsv\n","A mapping from a WordNet synset to a lemma. Each line contains a WordNet synset\n","ID and a lemma, separated by a tab. NOTE: the same WordNet synset ID may appear\n","in multiple lines. NOTE: the same lemma may appear in multiple lines.\n","For example:\n","    wn:00001740v    breathe\n","    wn:00001740v    take-a-breath\n","    wn:00002325v    respire\n","    wn:00002573v    respire\n","    ...\n","11. wn2sense.tsv\n","A mapping from a WordNet synset to a WordNet sense key. Each line contains a \n","WordNet synset ID and a WordNet sense key, separated by a tab. NOTE: the same\n","WordNet synset ID may appear in multiple lines.\n","For example:\n","    wn:00001740v    breathe%2:29:00::\n","    wn:00001740v    take_a_breath%2:29:00::\n","    wn:00002325v    respire%2:29:01::\n","    wn:00002573v    respire%2:29:02::\n","    ...\n","=================================================================================\n","CHANGELOG\n","=================================================================================\n","* VerbAtlas 1.1.0:\n","    * Merged some frames.\n","    * Changes to some predicate-argument structures.\n","    * Added frame definitions and small definitions.\n","    * Added prototypical synsets for each frame.\n","* VerbAtlas 1.0.3:\n","    * General, small fixes.\n","* VerbAtlas 1.0.2:\n","    * [NEW] Added VA_va2sp.tsv for frame-level selectional preferences.\n","    * Fix some typos in VA_bn2shadow.tsv (thanks Maurizio!)\n","    * Fix an incorrect synset in VA_bn2shadow.tsv\n","* VerbAtlas 1.0.1:\n","    * Fix missing roles for some frames (thanks German!)\n","* VerbAtlas 1.0:\n","    * Initial release\n","=================================================================================\n","REFERENCE PAPER\n","=================================================================================\n","When using this resource, please refer to the following paper:\n","    Andrea Di Fabio, Simone Conia and Roberto Navigli\n","    \"VerbAtlas: a Novel Large-Scale Verbal Semantic Resource\n","    and Its Application to Semantic Role Labeling\"\n","    In Proceedings of the 2019 Conference on Empirical Methods in Natural \n","    Language Processing and the 9th International Joint Conference on \n","    Natural Language Processing (EMNLP-IJCNLP),\n","    Hong Kong, China, November 3-7, 2019, pages 627-637.\n","=================================================================================\n","CONTACTS\n","=================================================================================\n","If you have any enquiries, please contact:\n","Andrea Di Fabio - Sapienza Università di Roma\n","(difabio [at] di [dot] uniroma1 [dot] it)\n","Simone Conia - Sapienza Università di Roma\n","(conia [at] di [dot] uniroma1 [dot] it)\n","Roberto Navigli - Sapienza Università di Roma\n","(navigli [at] di [dot] uniroma1 [dot] it)\n"]}],"source":["class VerbAtlas():\n","    def __init__(self, folder_path):\n","        # VA_bn2va.tsv: lists the BabelNet synsets in each VerbAtlas frame\n","        self.bn2va = self.__read_tsv(folder_path + \"/VerbAtlas-1.1.0/VA_bn2va.tsv\")\n","        self.bn_va_dict = [{}, defaultdict(lambda : [])]\n","        for bn2va_i in self.bn2va:\n","            self.bn_va_dict[0][bn2va_i[0]] = bn2va_i[1]\n","            self.bn_va_dict[1][bn2va_i[1]].append(bn2va_i[0])\n","            \n","        # bn2wn.tsv: mapping from BabelNet to WordNet\n","        self.bn2wn = self.__read_tsv(folder_path + \"/VerbAtlas-1.1.0/bn2wn.tsv\")\n","        self.bn_wn_dict = {}\n","        for bn2wn_i in self.bn2wn:\n","            self.bn_wn_dict[bn2wn_i[0]] = [bn2wn_i[1][3:-1], bn2wn_i[1][-1]]\n","\n","        # VA_frame_info.tsv: mapping from VerbAtlas ID to all the frame information, \n","        # including the frame name (second field)\n","        self.frame_info = self.__read_tsv(folder_path + \"/VerbAtlas-1.1.0/VA_frame_info.tsv\")\n","        self.frame_info_dict = {}\n","        for frame_info_i in self.frame_info:\n","            self.frame_info_dict[frame_info_i[0]] = frame_info_i[1:]\n","\n","        # README of VerbAtlas\n","        with open(folder_path + \"README.md\") as file:\n","            my_file = csv.reader(file, delimiter=\"\\t\")\n","            readme = \"\\n\".join([line[0] for line in my_file if line != []])\n","            print(readme)\n","\n","    def __read_tsv(self, path):\n","        # Read a file from VerbAtlas\n","        tsv_file = []\n","        with open(path) as file:\n","            my_file = csv.reader(file, delimiter=\"\\t\")\n","            for line in list(my_file)[1:]:\n","                tsv_file.append(line)\n","        return tsv_file\n","\n","# Verbatlas frames\n","vf = VerbAtlas(\"./verbatlas/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03HR_fUlZ5wN"},"outputs":[],"source":["def pprint(obj):\n","    '''\n","        Indented print of an object\n","    '''\n","    print(json.dumps(obj, indent=3))\n","    \n","def ambiguities(synset2frames):\n","    '''\n","        Separates ambiguous nominal synsets from unambiguous ones. \n","        Ambiguous means those nominal synsets derived from two different verbs \n","        associated with two different VerbAtlas frames.\n","        Input:\n","            synset2frames: \n","                dictionary with: {nominal_synset: VerbAtlas_frames_set}\n","        Output:\n","            ambiguous: \n","                Dictionary like input but containing only ambiguous synsets\n","            not_ambiguous: \n","                Dictionary like input but containing only unambiguous synsets\n","    '''\n","    ambiguous = {}\n","    not_ambiguous = {}\n","    for nominal_synset, verbatlas_frames in synset2frames.items():\n","        if len(verbatlas_frames) > 1:\n","            # More than one frame associated with this nominal synset.\n","            ambiguous[nominal_synset] = verbatlas_frames\n","        else:\n","            # A set of only one value since is unambiguous\n","            not_ambiguous[nominal_synset] = list(verbatlas_frames)[0]\n","    return ambiguous, not_ambiguous\n","\n","def nominal_synset_derivation(vf):\n","    '''\n","        For each verb contained in BableNet (associated with a VerbAtlas frame) \n","        a 1-to-1 match is found in WordNet. From the verb in WordNet \n","        the derivationally related forms are derived and for each of these \n","        the derivation tree is traced back through the hypernyms to the \"Entity\" synset. \n","        At this point several statistics are captured.\n","        Input: \n","            vf: VerbAtlas object (all files that are needed in the form of dictionaries)\n","        Output:\n","            event_ambiguous_nominal_synset:\n","                {nominal_synset: VerbAtlas_frames_set} dictionary where nominal_synsets are: \n","                    - Those that have reached the \"Event\" synset by going up the hypernym tree. \n","                    - They are ambiguous.\n","            event_not_ambiguous_nominal_synset:\n","                {nominal_synset: VerbAtlas_frames_set} dictionary where nominal_synsets are: \n","                    - Those that have reached the \"Event\" synset by going up the hypernym tree. \n","                    - They are not ambiguous.\n","            not_event_ambiguous_nominal_synset:\n","                {nominal_synset: VerbAtlas_frames_set} dictionary where nominal_synsets are: \n","                    - Those that didn't reached the \"Event\" synset by going up the hypernym tree. \n","                    - They are not ambiguous.\n","            not_event_not_ambiguous_nominal_synset:\n","                {nominal_synset: VerbAtlas_frames_set} dictionary where nominal_synsets are: \n","                    - Those that didn't reached the \"Event\" synset by going up the hypernym tree. \n","                    - They are not ambiguous.\n","            final_identification_set:\n","                {nominal_synset: definition_pos_tag} dictionary where nominal_synsets are \n","                those that didn't reached the \"Event\" synset by going up the \n","                hypernym tree and definition_pos_tag is the universal pos tagging \n","                associated to synset's definition.\n","                    \n","    '''\n","    # event_nominal_synset -> frame\n","    event_nom_syn2va_frames = defaultdict(lambda : set())\n","    # not_event_nominal_synset -> frame\n","    not_event_nom_syn2va_frames = defaultdict(lambda : set())\n","\n","    # For each \"bablnet to wordnet\" verb.\n","    for bablenet_id, [wn_offset, wn_pos] in vf.bn_wn_dict.items():\n","        synset = wn.synset_from_pos_and_offset(wn_pos, int(wn_offset))\n","        # Lemmas from the verb synsets\n","        synset_lemmas = synset.lemmas()\n","        nominal_synsets = set()\n","        # Find nominal_synsets\n","        for synset_lemma in synset_lemmas:\n","            drfs = synset_lemma.derivationally_related_forms()\n","            for drf in drfs:\n","                pos = drf.synset().pos()\n","                if pos == \"n\":\n","                    nominal_synsets.add(drf.synset())\n","        # Derive the hypernym tree and save the path, for each nominal_synset\n","        for nominal_synset in nominal_synsets:            \n","            synsets_to_hypernym = set([nominal_synset])\n","            visited = set()\n","            while synsets_to_hypernym != set():\n","                synset_to_hypernym = synsets_to_hypernym.pop()\n","                visited.add(synset_to_hypernym)\n","                hypernyms = synset_to_hypernym.hypernyms()\n","                for hypernym in hypernyms:\n","                    lemmas = hypernym.lemmas()\n","                    for lemma in lemmas:\n","                        pos = lemma.synset().pos()\n","                        if pos == \"n\":\n","                            synsets_to_hypernym.add(lemma.synset())\n","            va_frame = vf.frame_info_dict[vf.bn_va_dict[0][bablenet_id]][0]\n","            # If a nomina_synset reach \"event\"\n","            if set(wn.synsets(\"event\")).intersection(visited) != set():\n","                event_nom_syn2va_frames[nominal_synset].add(va_frame)\n","            else:\n","                not_event_nom_syn2va_frames[nominal_synset].add(va_frame)\n","    final_identification_set = {}\n","    for synset in not_event_nom_syn2va_frames.keys(): \n","        final_identification_set[synset] = pos_tagging(synset.definition())\n","\n","    # Divide the ambiguous from the unambiguous\n","    event_ambiguous_nominal_synset, event_not_ambiguous_nominal_synset = ambiguities(event_nom_syn2va_frames)\n","    not_event_ambiguous_nominal_synset, not_event_not_ambiguous_nominal_synset = ambiguities(not_event_nom_syn2va_frames)\n","    return (event_ambiguous_nominal_synset, \n","            event_not_ambiguous_nominal_synset, \n","            not_event_ambiguous_nominal_synset,\n","            not_event_not_ambiguous_nominal_synset,\n","            final_identification_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0h5qOdqXvcM"},"outputs":[],"source":["###########\n","# UTILITY #\n","###########\n","def pos_tagging(sentence, split = True):\n","    '''\n","    This function genererate the universal pos tag of a sentence.\n","    Args:\n","        sentence: sentence to tag\n","        split (default True): if True, the sentence is already tokenized, otherwise\n","                              we need a tokenization\n","    Return:\n","        universal_pos_tags: list of [word, tag], where \"word\" is the word in the sentence\n","                            and \"tag\" is the universal pos tag associated\n","    '''\n","    pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence)) if split else nltk.pos_tag(sentence)\n","    universal_pos_tags = []\n","    for word, pos in pos_tags:\n","        if pos in [\"JJ\", \"JJR\", \"JJS\"]:\n","            universal_pos_tags.append([word, \"ADJ\"])\n","        elif pos in [\"IN\"]:\n","            universal_pos_tags.append([word, \"ADP\"])\n","        elif pos in [\"RB\", \"RBR\", \"RBS\"]:\n","            universal_pos_tags.append([word, \"ADV\"])\n","        elif pos in [\"MD\"]:\n","            universal_pos_tags.append([word, \"AUX\"])\n","        elif pos in [\"CC\"]:\n","            universal_pos_tags.append([word, \"CCONJ\"])\n","        elif pos in [\"DT\", \"PDT\", \"WDT\"]:\n","            universal_pos_tags.append([word, \"DET\"])\n","        elif pos in [\"UH\"]:\n","            universal_pos_tags.append([word, \"INTJ\"])\n","        elif pos in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n","            universal_pos_tags.append([word, \"NOUN\"])\n","        elif pos in [\"CD\"]:\n","            universal_pos_tags.append([word, \"NUM\"])\n","        elif pos in [\"POS\"]:\n","            universal_pos_tags.append([word, \"PART\"])\n","        elif pos in [\"PRP\", \"PRP$\", \"WP\", \"WP$\"]:\n","            universal_pos_tags.append([word, \"PRON\"])\n","        elif pos in [\"NNP\", \"NNPS\"]:\n","            universal_pos_tags.append([word, \"PROPN\"])\n","        elif pos in [\".\", \",\", \":\", \"!\", \"?\"]:\n","            universal_pos_tags.append([word, \"PUNCT\"])\n","        elif pos in [\"IN\"]:\n","            universal_pos_tags.append([word, \"SCONJ\"])\n","        elif pos in [\"SYM\"]:\n","            universal_pos_tags.append([word, \"SYM\"])\n","        elif pos in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n","            universal_pos_tags.append([word, \"VERB\"])\n","        else:\n","            universal_pos_tags.append([word, \"X\"])\n","    return universal_pos_tags\n","\n","def api_call_verbatlas(definition):\n","    '''\n","    This function makes an API call to VerbAtlas obtaining its evaluation. \n","    It then associates each word with either \"~\" or the frame if it is a verb.\n","    Args:\n","        definition: definition of a synset\n","    Return:\n","        idxs_and_vfs: list of [token_idx, frame_name] where token_idx is the index\n","                      of the verb in the sentece and frame_name is its associated frame.\n","                      With a response code of 500 this value is \"\"\n","        va_split_sent: the splitted sentence from the VerbAtlas response.\n","                       With a response code of 500 this value is \"\"\n","    '''\n","    # API call\n","    # If the character '\"' is present in the definition, the input string of \n","    # the API call will fail. For this reason, the escape character \"\\\" has been added, \n","    # which must appear in the request string and so '\"' has been replaced with '\\\"'\n","    definition = definition.replace('\"', '\\\\\"')\n","    input_str = '[{\"text\":\"' + definition + '\", \"lang\":\"EN\"}]'\n","    # We convert the input string to JSON format\n","    input_json = json.loads(input_str)\n","    # We define the parameters of the HTTP request\n","    url = 'https://verbatlas.org/api/model'\n","    headers = {'accept': 'application/json', 'Content-Type': 'application/json'}\n","    # We execute the HTTP request using the POST method and passing the parameters defined\n","    response = requests.post(url, headers=headers, json=input_json)\n","    \n","    # Postprocessing of the response\n","    idxs_and_vfs = \"\"\n","    va_split_sent = \"\"\n","    if response.status_code != 500:\n","        response_json = json.loads(response.text)\n","        va_split_sent = [x[\"rawText\"] for x in response_json[0][\"tokens\"]]\n","        idxs_and_vfs = []\n","        for annotation in response_json[0][\"annotations\"]:\n","            frame_name = annotation[\"verbatlas\"][\"frameName\"]\n","            token_index = annotation[\"tokenIndex\"]\n","            idxs_and_vfs.append([token_index, frame_name])\n","    return idxs_and_vfs, va_split_sent\n","\n","################################\n","# EVENT IDENTIFICATION DATASET #\n","################################\n","def compute_event_set():\n","    '''\n","    This function is used to obtain all nominal event synsets generated from the synsets \"Event\"\n","    Return:\n","        - list of all event synsets to use for the datasets\n","    '''\n","    synset_event = wn.synsets(\"event\")\n","    event_set = set()\n","    # visitare l'albero di hyponyms ricorsivamente e aggiungere i synset nominali alla lista\n","    def get_hyponyms(synset):\n","        '''\n","        Recursive function that add all the hyponyms of a synset\n","        Args:\n","            synset: synset from which hyponyms will be obtained\n","        '''\n","        if synset.pos() == 'n':\n","            event_set.add(synset)\n","        for hyp in synset.hyponyms():\n","            get_hyponyms(hyp)\n","\n","    for event_s in synset_event:\n","        get_hyponyms(event_s)\n","    return list(event_set)\n","\n","def compute_not_event_set(event_set, \n","                          not_ev):\n","    '''\n","    This function is used to obtain all nominal not event synsets generated \n","    from specific synsets that we are sure are not event.\n","    Args:\n","        event_set: are the synsets to be classified as event\n","        not_ev: are the synsets that have not reached the \"Event\" synsets, ambiguous and not\n","    Return:\n","        not_event_set: list of all not event synsets to use for the datasets\n","    '''\n","    # Synsets as general as possible considered as not event through a manual analysis of the definition\n","    synset_list = ['thing.n.08', 'object.n.01', 'set.n.02', 'substance.n.04', 'matter.n.03', 'otherworld.n.01', 'measure.n.02', 'group.n.01', 'attribute.n.02']\n","\n","    # Set of not event synsets derived from one of the synset_list\n","    synset_set = set()\n","\n","    def get_hyponyms(synset):\n","        '''\n","        Recursive function that add all the hyponyms of a synset\n","        Args:\n","            synset: synset from which hyponyms will be obtained\n","        '''\n","        if synset.pos() == 'n':\n","            synset_set.add(synset)\n","        for hyp in synset.hyponyms():\n","            get_hyponyms(hyp)\n","\n","    # Get not event synsets from the hyponym tree of the synsets in synset_list\n","    before = 0\n","    synset_dict = {}\n","    for synset_name in synset_list:\n","        synset = wn.synset(synset_name)\n","        get_hyponyms(synset)\n","        synset_dict[synset_name] = synset_set\n","        synset_set = set()\n","        \n","    list_of_lists = [list(synsets) for synsets in list(synset_dict.values())]\n","    not_event_set = [list(set(itertools.chain(*list_of_lists)) & not_ev)]\n","    rest_not_event = set(itertools.chain(*list_of_lists)) - set(not_event_set[0])\n","    num_not_event_synsets = len(rest_not_event)\n","    for key, value in synset_dict.items():\n","        synset_dict[key] = value - (value & set(not_event_set[0]))\n","    \n","    # Number of not event synset to select from the list for each geeneral synset\n","    division = int((len(event_set) - len(not_event_set[0]) - 34)/ 5)\n","\n","    for i, (key, value) in enumerate(synset_dict.items()):\n","        # The value 26 was found manually looking at the frequencies. If the frequency is\n","        # under this value, then all the synsets will be added, otherwise only \n","        # \"division\" synsets will be added\n","        if len(value) > 26: \n","            value_list = list(value)\n","            r = random.sample(value_list, division)\n","            not_event_set.append(r) \n","        else:\n","            not_event_set[0] += list(value)\n","    return not_event_set\n","\n","def set_dict_identification(event_set, not_event_set):\n","    '''\n","    This function generates the dataset in the correct format\n","    Args:\n","        event_set: all event synsets\n","        not_event_set: all not event synsets\n","    Return:\n","        mset: dataset with the format: \n","              {synset_name: [pos_tagging(synset_definition), label]}\n","    '''\n","    mset = {}\n","    for synset_e, synset_ne in zip(event_set, not_event_set):\n","        mset[str(synset_e)] = [pos_tagging(synset_e.definition()), \"event\"]\n","        mset[str(synset_ne)] = [pos_tagging(synset_ne.definition()), \"not_event\"]\n","    return mset\n","\n","def identification_dataset_creation(final_identification_set,\n","                                    not_event_not_ambiguous_nominal_synset,\n","                                    not_event_ambiguous_nominal_synset):\n","    '''\n","    This function generates, for the identification step, the datasets: \n","    train, dev, test and final. The final dataset contains all the not event synsets\n","    that could be event\n","    Args: \n","        final_identification_set: all the not event synsets\n","        not_event_not_ambiguous_nominal_synset: all not event unambiguous synsets\n","        not_event_ambiguous_nominal_synset: all not event ambiguous synsets\n","    '''\n","    # Datasets ~[70-15-15]%\n","    sets = {\"train\": [], \"dev\": [], \"test\": [], \"final\": []}\n","\n","    # Event synsets\n","    event_set = compute_event_set()\n","    # Not event synsets\n","    not_ev_amb = set(list(not_event_ambiguous_nominal_synset.keys()))\n","    not_ev_not_amb = set(list(not_event_not_ambiguous_nominal_synset.keys()))\n","    not_event_set = compute_not_event_set(event_set, set(list(not_ev_amb) + list(not_ev_not_amb)))\n","    # Not event synset for the final dataset\n","    final_not_event_set = set(list(final_identification_set.keys()))\n","    support_list = []\n","    for not_events in not_event_set:\n","        support_list += not_events\n","    final_not_event_set = final_not_event_set - (final_not_event_set & set(support_list))\n","\n","    # Training set\n","    t_p = 0.7\n","    event_t = random.sample(list(event_set), int(len(event_set)*t_p))\n","    not_event_t = []\n","    not_train_not_event = []\n","    for not_event in not_event_set:        \n","        not_event_t += random.sample(list(not_event), int(len(not_event)*t_p))\n","        not_train_not_event.append(list(set(not_event) - set(not_event_t)))\n","    sets[\"train\"] = set_dict_identification(event_t, not_event_t)\n","    \n","    # Developement set\n","    not_train_event = list(set(event_set) - set(event_t))\n","    event_d = random.sample(not_train_event, int(len(not_train_event)*0.5))\n","    not_event_d = []\n","    not_dev_not_event = []\n","    for not_event in not_train_not_event:        \n","        not_event_d += random.sample(list(not_event), int(len(not_event)*0.5))\n","        not_dev_not_event.append(list(set(not_event) - set(not_event_d)))\n","    sets[\"dev\"] = set_dict_identification(event_d, not_event_d)\n","    \n","    # Test set\n","    event_te = list(set(not_train_event) - set(event_d))\n","    not_event_te = []\n","    for not_event in not_dev_not_event:\n","        not_event_te += not_event\n","    sets[\"test\"] = set_dict_identification(event_te, not_event_te)\n","\n","    # Final set\n","    mset = {}\n","    for final in final_not_event_set:\n","        mset[str(final)] = pos_tagging(final.definition())\n","    sets[\"final\"] = mset\n","\n","    # Save\n","    with open(DATASETS_PATH + \"dataset_identification.json\", \"w\") as f:\n","        json.dump(sets, f)\n","\n","################################\n","# EVENT CLASSIFICATION DATASET #\n","################################\n","def set_dict_classification(synsets, all_synset):\n","    '''\n","    This function generates the dataset in the correct format\n","    Args:\n","        synsets: all synsets for a specific dataset (train, dev, test or final)\n","        all_synset: it's a dictionary containing all ambiguous or unambiguous synsets, \n","                    where the key is the name of the synset and \n","                    the value is the class list or class belonging (verbatlas frames).\n","    Return:\n","        mset: dataset with the format: \n","              {synset_name: [pos_tagging(synset_definition), label, VerbAtlas_computation]}\n","    '''\n","    mset = {}\n","    synset_list = list(synsets)\n","    for synset in synset_list:\n","        vf_d = \"~\"\n","        idx_and_vf, va_split_sent = api_call_verbatlas(synset.definition())\n","        if idx_and_vf == \"\":\n","            pos_tag = pos_tagging(synset.definition())\n","            for word in pos_tag:\n","                word.append(vf_d)\n","        else:\n","            pos_tag = pos_tagging(va_split_sent, split = False)\n","            for idx, vf in idx_and_vf:\n","                if vf == \"_\":\n","                    vf = vf_d\n","                pos_tag[idx].append(vf)\n","            for word in pos_tag:\n","                if len(word) < 3:\n","                    word.append(vf_d)\n","        classes = all_synset[synset]\n","        if type(all_synset[synset]) == set:\n","            classes = list(classes)\n","        mset[str(synset)] = [pos_tag, classes]\n","    return mset\n","\n","def classification_dataset_creation(event_not_ambiguous_nominal_synset, \n","                                    event_ambiguous_nominal_synset,\n","                                    extra):\n","    '''\n","    This function generates, for the classification step, the datasets: \n","    train, dev, test and final. The final dataset contains all the ambiguous synsets\n","    that could be disambiguated\n","    Args: \n","        event_not_ambiguous_nominal_synset: all the event unambiguous synsets\n","        event_ambiguous_nominal_synset: all not event ambiguous synsets\n","        extra: all event synsets, ambiguous and not, found in the identification step\n","    '''\n","    # 1. Datasets ~[80-10-10]%\n","    sets = {}\n","\n","    # 2. Include the extra synsets\n","    ambiguous = {}\n","    for synset, classes in list(extra[\"ambiguous\"].items()) + list(event_ambiguous_nominal_synset.items()):\n","        ambiguous[synset] = classes\n","    unambiguous = {}\n","    for synset, classes in list(extra[\"unambiguous\"].items()) + list(event_not_ambiguous_nominal_synset.items()):\n","        unambiguous[synset] = classes\n","\n","    # 3. Generate the labels\n","    # Get frequences for each frame of the unambiguous synsets\n","    statistics = Counter(list(unambiguous.values()))\n","    # Labels dictionary {VerbAtlas_frame: [frequency, synsets_list]}\n","    labels = {}\n","    # Special class \"other\" for all frames with low frequency\n","    # This class is a dictionary {VerbAtlas_frame: [frequency, synsets_list]}\n","    labels[\"other\"] = {}\n","    # The other class has a special class \"few_occurences\" that merge in a single\n","    # class all the frame with frequency less than 3 because they can't be splitted\n","    # in train, dev and test sets. \n","    # This class has associated a triple [frequencey, frames_list, synsets_list]\n","    labels[\"other\"][\"few_occurrences\"] = [0, [], []]\n","    for frame, occurrences in statistics.items():\n","        if occurrences >= 11:\n","            labels[frame] = [occurrences, []]\n","        else:\n","            if occurrences < 3:\n","                labels[\"other\"][\"few_occurrences\"][0] += occurrences\n","                labels[\"other\"][\"few_occurrences\"][1].append(frame)\n","            labels[\"other\"][frame] = [occurrences, []]\n","\n","    # 4. Manipulating the sets by including the label \"other\" instead of low-frequency frames \n","    # and adding the synsets to the corresponding label. \n","    # 4.1. For unambiguous synsets, if the associated frame is present in the label list, \n","    # it remains unchanged. On the other hand, if it is not present, the label \"other\" \n","    # is associated with this synset.\n","    unambiguous_valid = {}\n","    for synset, frame in unambiguous.items():\n","        if frame in list(labels.keys()):\n","            labels[frame][1].append(synset)\n","            unambiguous_valid[synset] = frame\n","        else:\n","            if frame in labels[\"other\"][\"few_occurrences\"][1]:\n","                labels[\"other\"][\"few_occurrences\"][2].append(synset)\n","            labels[\"other\"][frame][1].append(synset)\n","            unambiguous_valid[synset] = \"other\"\n","    # 4.2. For ambiguous synsets, if the list of frames associated with it has \n","    # at least all frames minus one within the non-\"other\" labels, then the pair [synset, frames] is kept.\n","    # In this way, all ambiguous synsets that have at most one frame of ambiguity \n","    # in the class \"other\" will be kept, so that at the disambiguation step it is possible to determine \n","    # which is the correct frame to associate to that synset, and, in addition, in this way, \n","    # it was possible to aggregate most of the less frequent frames \n","    # into a single class managing to minimize the synsets to be discarded.\n","    ambiguous_valid = {}\n","    for synset, frames in ambiguous.items():\n","        if len(set(list(labels.keys())).intersection(set(frames))) >= len(frames) - 1:\n","            ambiguous_valid[synset] = frames\n","\n","    # 5. Train set\n","    t_p = 0.8\n","    train_set_synset = []\n","    sampled = False\n","    # For each frame, the training set gets the t_p% of the synsets. If the label is \"other\", \n","    # the training set takes the t_p% of all frames in the label \"other\". For the label \"few_occurrences\" \n","    # in the label \"other,\" where all frames with less than three synsets are merged into one set, \n","    # the t_p% was taken once from this merged set. In this way, from all types of frames \n","    # the t_p% of samples were taken so as to balance between train, dev and test set \n","    # respecting the splitting percentages (in this case [80-10-10]%)\n","    for label, synsets in labels.items():\n","        if label == \"other\":\n","            for label_oth, synsets_oth in synsets.items():\n","                if label_oth == \"few_occurrences\":\n","                    continue\n","                if label_oth in labels[\"other\"][\"few_occurrences\"][1]:\n","                    if sampled:\n","                        continue\n","                    population = labels[\"other\"][\"few_occurrences\"][2]\n","                    sample = random.sample(population, int(len(population)*t_p))\n","                    sampled = True\n","                else:\n","                    sample = random.sample(synsets_oth[1], int(len(synsets_oth[1])*t_p))\n","                    if synsets_oth[0] - len(sample) < 2:\n","                        sample = random.sample(sample, len(sample) - (synsets_oth[0] - len(sample)))\n","                train_set_synset += sample\n","        else:\n","            sample = random.sample(synsets[1], int(len(synsets[1])*t_p))\n","            train_set_synset += sample\n","    train_set = set_dict_classification(train_set_synset, unambiguous_valid)\n","    sets[\"train\"] = train_set\n","\n","    # 6. Developement set\n","    dev_set_synset = []\n","    rest_set_synset = list(unambiguous_valid.keys() - set(train_set_synset))\n","    sampled = False\n","    # Same reasoning as Train set (see section 5.) with the difference that half of the synsets, \n","    # for each type of frame, are taken from the remaining synsets by removing those included in the Train set\n","    for label, synsets in labels.items():\n","        if label == \"other\":\n","            for label_oth, synsets_oth in synsets.items():\n","                if label_oth == \"few_occurrences\":\n","                    continue\n","                if label_oth in labels[\"other\"][\"few_occurrences\"][1]:\n","                    if sampled:\n","                        continue\n","                    population = labels[\"other\"][\"few_occurrences\"][2]\n","                    dev_synsets = set(population) - (set(population) - set(rest_set_synset))\n","                    sample = random.sample(list(dev_synsets), int(len(dev_synsets)*0.5))\n","                    sampled = True\n","                else:\n","                    dev_synsets = set(synsets_oth[1]) - (set(synsets_oth[1]) - set(rest_set_synset))\n","                    sample = random.sample(list(dev_synsets), int(len(dev_synsets)*0.5))      \n","                dev_set_synset += sample\n","        else:\n","            dev_synsets = set(synsets[1]) - (set(synsets[1]) - set(rest_set_synset))\n","            sample = random.sample(list(dev_synsets), int(len(dev_synsets)*0.5))\n","            dev_set_synset += sample\n","    dev_set = set_dict_classification(dev_set_synset, unambiguous)\n","    sets[\"dev\"] = dev_set\n","    \n","    # 7. Test set\n","    # All remaining synsets after those inserted in Train and Dev sets (see sections 5. and 6.)\n","    test_set_synset = list(set(rest_set_synset) - set(dev_set_synset))\n","    test_set = set_dict_classification(test_set_synset, unambiguous)\n","    sets[\"test\"] = test_set\n","\n","    # 8. Final set\n","    # All ambiguous synsets that have passed the skimming of the class \"other\" (see section 4.2.)\n","    final_set_synset = list(ambiguous_valid.keys())\n","    final_set = set_dict_classification(final_set_synset, ambiguous)\n","    sets[\"final\"] = final_set\n","\n","    # 9. Save\n","    with open(DATASETS_PATH + \"dataset_classification.json\", \"w\") as f:\n","        json.dump(sets, f)\n","    with open(DATASETS_PATH + \"labels_classification.json\", \"w\") as f:\n","        json.dump(list(labels.keys()), f)\n","    with open(DATASETS_PATH + \"other_labels_classification.json\", \"w\") as f:\n","        json.dump(list(set(list(labels[\"other\"].keys())) - set(['few_occurrences'])), f)\n","\n","def extra_synsets_from_identification(not_event_ambiguous_nominal_synset, \n","                                      not_event_not_ambiguous_nominal_synset):\n","    '''\n","    This function read the extra synsets found in the identification step.\n","    Args:\n","        not_event_ambiguous_nominal_synset: all not event ambiguous synsets\n","        not_event_not_ambiguous_nominal_synset: all not event unambiguous synsets\n","    Return:\n","        extra: all synsets obtained in the identification step divided into ambiguous and unambiguous\n","    '''\n","    with open(OUTPUT_PATH + \"event_identification_step_results.json\", \"r\") as file:\n","        event_identification_step = json.load(file)\n","    extra = {\"ambiguous\": {}, \"unambiguous\": {}}\n","    for synset_str, id in event_identification_step.items():\n","        if id == \"event\":\n","            synset = wn.synset(synset_str)\n","            if not_event_ambiguous_nominal_synset.get(synset) is not None:\n","                extra[\"ambiguous\"][synset] = list(not_event_ambiguous_nominal_synset.get(synset))\n","            else:\n","                extra[\"unambiguous\"][synset] = not_event_not_ambiguous_nominal_synset.get(synset)\n","    return extra"]},{"cell_type":"markdown","source":["#Dataset creation"],"metadata":{"id":"mozm3_TBM4iR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n43MsgwcJrON"},"outputs":[],"source":["# Extrapolates all information about event, nonevent, ambiguous, and unambiguous synsets \n","# through the link between WordNet and VerbAtlas\n","random.seed(104)\n","event_ambiguous_nominal_synset,             \\\n","event_not_ambiguous_nominal_synset,         \\\n","not_event_ambiguous_nominal_synset,         \\\n","not_event_not_ambiguous_nominal_synset,     \\\n","final_identification_set                    = nominal_synset_derivation(vf)"]},{"cell_type":"code","source":["# Creates the Dataset for the Identification step\n","identification_dataset_creation(final_identification_set,\n","                                not_event_not_ambiguous_nominal_synset,\n","                                not_event_ambiguous_nominal_synset)"],"metadata":{"id":"nPJP1_ddh2Uo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oC86ErfUsUz"},"outputs":[],"source":["# Creates the Dataset for the Classification step\n","extra = extra_synsets_from_identification(not_event_ambiguous_nominal_synset, \n","                                          not_event_not_ambiguous_nominal_synset)\n","classification_dataset_creation(event_not_ambiguous_nominal_synset, \n","                                event_ambiguous_nominal_synset,\n","                                extra)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}